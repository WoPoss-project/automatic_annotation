{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test-stanza.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE0megDJPpt-"
      },
      "source": [
        "# Automatic annotation of classical languages\n",
        "\n",
        "Some introductory words...\n",
        "\n",
        "In this session we would use Stanza developed by the Stanford NLP group, and spaCy to visualize the dependencies.\n",
        "\n",
        "Performance of the system on different UD Treebanks is available in the [website of the project](https://stanfordnlp.github.io/stanza/performance.html).\n",
        "\n",
        "\n",
        "\n",
        "## Quick introduction to Jupyter Notebooks\n",
        "\n",
        "First, a Jupyter Notenook is made of cells. The borders of a cell appear when you click on it.\n",
        "A cell can contain:\n",
        "- Text in Markdown (like this cell)\n",
        "- Code in Python (like most of the cells of this notebook)\n",
        "\n",
        "What you see in the cells with codes in green (they all begin with a `#`) are comments. Everything else is code. We divided the code in different cells so you can run the code piece by piece. \n",
        "To **run a cell** just click on it and the press on the “play” button at the upper left of each cell (alternatively, press `Shift + Enter`). Depending on what the code does, you might see some output being generated below the cell. \n",
        "\n",
        "### Things to consider\n",
        "\n",
        "- Be patient: the installation might take some seconds and downloading the models might take up to a couple of minutes.\n",
        "- Be aware of the order: if you try to run the cell where we display the results of the dependencies without running first the installation, the importation of the modules, the pipeline of the linguistic analysis... it is not going to work. You only need to install and dowload things once, but if, for instance, you change the text you want to analyse, you need to rerun the cell that contains the text and then the cells below with the analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHYL_tcEQJjo"
      },
      "source": [
        "#Import the library\n",
        "import stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTjc9ZJoQsDV"
      },
      "source": [
        "#Download the greek models\n",
        "stanza.download(lang='grc', package='proiel') #this one is the default model if we don't specify a package\n",
        "stanza.download(lang='grc', package='perseus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvQozjdtT8DI"
      },
      "source": [
        "#Let's put some text in a variable\n",
        "text = \"Ἐὰν ᾖς φιλομαθής, ἔσει πολυμαθής\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WadGIPt-VTkU"
      },
      "source": [
        "#First, we initialize a pipeline, which preloads and chains up a series of processors. \n",
        "#Each one of this processors performs a NLP task (e.g., lemmatization, dependency parsing, etc.)\n",
        "nlp = stanza.Pipeline(lang='grc', package=\"proiel\")\n",
        "#And we can already pass our text to the pipeline for it to be annotated\n",
        "doc = nlp(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxNco9G7X6el"
      },
      "source": [
        "#Before printing the annotation, we access each sentence, and each word of each sentence\n",
        "for sentence in doc.sentences:\n",
        "  for word in sentence.words:\n",
        "#Now, we are goint to print the lemmata (with some text in between so it is more legible):\n",
        "    print(word.text, \"->\", word.lemma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD6e7TwrafiE"
      },
      "source": [
        "#Now let's do both the part of the speech and the lemma\n",
        "for sentence in doc.sentences:\n",
        "  for word in sentence.words:\n",
        "    print(word.text, \" lemma:\", word.lemma, \" PoS:\", word.pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI4XF7HPbtUg"
      },
      "source": [
        "#Morphological features\n",
        "for sentence in doc.sentences:\n",
        "  for word in sentence.words:\n",
        "    print(word.text, word.feats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Fgq9XdcR9e"
      },
      "source": [
        "#If we print the dependencies, we get a JSON file. For each token, we see all its properties, including the dependencies: \n",
        "#the \"head\" of the token, and the \"deprel\", the relation between the token and its head.\n",
        "for sentence in doc.sentences:\n",
        "  print(sentence.dependencies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gGmab2qUq8z"
      },
      "source": [
        "#Visualization of the dependencies using spaCy\n",
        "#Firt, we import the required packages\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from spacy_stanza import StanzaLanguage\n",
        "#And we initialize the pipeline\n",
        "snlp = stanza.Pipeline(lang=\"grc\", package='proiel')\n",
        "nlp = StanzaLanguage(snlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3hcKAQnccc6"
      },
      "source": [
        "#We pass our text through the pipeline (remember: the variable text was declared a few cells above)\n",
        "doc = nlp(text)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DfxhowidMh-"
      },
      "source": [
        "#Now we are going to do the same using the perseus model\n",
        "snlp = stanza.Pipeline(lang=\"grc\", package='perseus')\n",
        "nlp = StanzaLanguage(snlp)\n",
        "doc = nlp(text)\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}